{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0dcb74",
   "metadata": {},
   "source": [
    "데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d4b266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import glob  \n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bda2615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os, re \n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*' #os.getenv(x)함수는 환경 변수x의 값을 포함하는 문자열 변수를 반환합니다. txt_file_path 에 \"/root/aiffel/lyricist/data/lyrics/*\" 저장\n",
    "\n",
    "txt_list = glob.glob(txt_file_path) #txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list 에 할당\n",
    "\n",
    "raw_corpus = [] \n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
    "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ac997",
   "metadata": {},
   "source": [
    "데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb715ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   \n",
    "    if sentence[-1] == \":\": continue \n",
    "\n",
    "    if idx > 9: break .\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68117e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7d5bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    # preprocess_sentence() 함수를 이용하여 토큰 개수가 15개 미만인 문장만 저장\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if len(preprocessed_sentence.split(' '))<=15:\n",
    "        corpus.append(preprocessed_sentence)\n",
    "\n",
    "# 결과 확인\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e767616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156013\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c8273e",
   "metadata": {},
   "source": [
    "평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae043a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2967 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  118 ...    0    0    0]\n",
      " [   2  258  194 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f9e5d3c42b0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 12000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c98df753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4   95  303   62   53    9  946 6263]\n",
      " [   2   15 2967  871    5    8   11 5739    6  374]\n",
      " [   2   33    7   40   16  164  288   28  333    5]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41dde504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45311d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    4   95  303   62   53    9  946 6263    3    0    0    0]\n",
      "[  50    4   95  303   62   53    9  946 6263    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "tgt_input = tensor[:, 1:]  # tensor에서 <start>를 잘라내서 타겟 문장을 생성 \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59fadc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris # 샘플 데이터 로딩\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "src_input = tensor[:, :-1]\n",
    "tgt_input = tensor[:, 1:]  \n",
    "\n",
    "X = src_input\n",
    "y = tgt_input\n",
    "\n",
    "# train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da1494e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124810, 14) (124810, 14)\n",
      "(31203, 14) (31203, 14)\n"
     ]
    }
   ],
   "source": [
    "print(enc_train.shape, dec_train.shape)\n",
    "print(enc_val.shape, dec_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47aa3fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b31e218",
   "metadata": {},
   "source": [
    "인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5e6b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "       . \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "embedding_size = 256 \n",
    "hidden_size = 1024 \n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4b50c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 5.62852038e-05, -5.74638725e-05, -5.54467151e-05, ...,\n",
       "         -1.04926170e-04, -1.92459606e-06,  6.67352433e-05],\n",
       "        [-2.50754656e-05, -1.41379511e-04,  1.27630790e-06, ...,\n",
       "         -3.31160845e-04, -1.90512044e-04,  1.93272514e-04],\n",
       "        [-1.85434968e-04, -6.57441633e-05,  1.92020205e-04, ...,\n",
       "         -6.64999767e-04, -2.57447653e-04,  2.01440285e-04],\n",
       "        ...,\n",
       "        [ 5.66817529e-04,  1.67906587e-03,  1.47010793e-03, ...,\n",
       "         -1.92827731e-03, -1.33572286e-03,  1.68988667e-03],\n",
       "        [ 9.83903068e-04,  1.94801332e-03,  1.24063471e-03, ...,\n",
       "         -1.81187200e-03, -1.55316794e-03,  2.00679689e-03],\n",
       "        [ 1.36897736e-03,  2.15465738e-03,  9.79853445e-04, ...,\n",
       "         -1.67081528e-03, -1.73514348e-03,  2.29967688e-03]],\n",
       "\n",
       "       [[ 5.62852038e-05, -5.74638725e-05, -5.54467151e-05, ...,\n",
       "         -1.04926170e-04, -1.92459606e-06,  6.67352433e-05],\n",
       "        [ 2.47048330e-04,  2.55967403e-04,  4.69011638e-06, ...,\n",
       "         -2.26988588e-04,  1.42269637e-04,  1.13911883e-04],\n",
       "        [ 8.71278971e-05,  2.19213296e-04, -5.10942446e-05, ...,\n",
       "         -1.19755307e-04,  3.38056008e-04, -6.03734006e-05],\n",
       "        ...,\n",
       "        [ 1.35638722e-04, -2.83616669e-06, -1.20591617e-03, ...,\n",
       "          6.40195853e-04,  7.27434584e-04,  9.59773082e-04],\n",
       "        [-5.22519731e-05,  1.08188426e-04, -9.85957100e-04, ...,\n",
       "          4.94981825e-04,  5.14204730e-04,  9.87193896e-04],\n",
       "        [ 1.13244614e-05,  4.12171270e-04, -7.74217537e-04, ...,\n",
       "          2.21861104e-04,  1.47770406e-04,  1.17071567e-03]],\n",
       "\n",
       "       [[ 5.62852038e-05, -5.74638725e-05, -5.54467151e-05, ...,\n",
       "         -1.04926170e-04, -1.92459606e-06,  6.67352433e-05],\n",
       "        [-3.39413848e-04, -1.07213717e-04, -2.02407187e-04, ...,\n",
       "         -8.19432171e-05, -1.62020864e-04, -2.63168804e-05],\n",
       "        [-9.00504412e-04, -1.72424698e-04, -2.29549449e-04, ...,\n",
       "         -3.80765967e-04, -2.97788156e-05, -1.70827843e-04],\n",
       "        ...,\n",
       "        [ 5.15805325e-04,  7.37961382e-04,  1.27209642e-03, ...,\n",
       "         -1.23849732e-03, -8.67580296e-04,  1.98231474e-03],\n",
       "        [ 9.53177048e-04,  9.88796819e-04,  1.16464263e-03, ...,\n",
       "         -1.28750154e-03, -1.15485361e-03,  2.34893733e-03],\n",
       "        [ 1.37084362e-03,  1.21569517e-03,  9.90828266e-04, ...,\n",
       "         -1.28389348e-03, -1.39903731e-03,  2.65203090e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 5.62852038e-05, -5.74638725e-05, -5.54467151e-05, ...,\n",
       "         -1.04926170e-04, -1.92459606e-06,  6.67352433e-05],\n",
       "        [ 3.04071931e-04,  6.15784529e-06, -2.90945696e-04, ...,\n",
       "         -1.60115334e-04,  1.47065832e-04,  2.24488467e-04],\n",
       "        [ 4.13990900e-04, -2.05326927e-04, -1.91059124e-04, ...,\n",
       "         -1.28824240e-05,  2.33651328e-04,  2.42502691e-04],\n",
       "        ...,\n",
       "        [-7.78628921e-04,  9.83363250e-04,  5.50093711e-04, ...,\n",
       "         -1.22095633e-03, -3.14868434e-04,  1.35641324e-03],\n",
       "        [-8.22288974e-04,  1.10062480e-03,  6.17479673e-04, ...,\n",
       "         -1.38062518e-03, -7.41858850e-04,  1.43608614e-03],\n",
       "        [-6.31615170e-04,  1.25295494e-03,  5.72958437e-04, ...,\n",
       "         -1.52089505e-03, -1.14533259e-03,  1.60738267e-03]],\n",
       "\n",
       "       [[ 5.62852038e-05, -5.74638725e-05, -5.54467151e-05, ...,\n",
       "         -1.04926170e-04, -1.92459606e-06,  6.67352433e-05],\n",
       "        [ 9.99884287e-05, -1.42075893e-04, -2.47222924e-04, ...,\n",
       "          1.59592062e-04, -2.00856302e-04,  4.97687943e-05],\n",
       "        [ 1.44405887e-04, -3.53503019e-05, -4.30929533e-04, ...,\n",
       "          3.28484486e-04, -1.38051677e-04, -2.49193690e-04],\n",
       "        ...,\n",
       "        [-2.28106030e-04,  5.83296176e-04, -2.03890479e-04, ...,\n",
       "         -1.06321869e-03, -4.74834058e-04,  8.71771408e-05],\n",
       "        [ 2.55522871e-04,  9.11816198e-04, -2.25921176e-04, ...,\n",
       "         -1.32684165e-03, -8.75028141e-04,  5.91188029e-04],\n",
       "        [ 7.55073328e-04,  1.20381382e-03, -2.86641327e-04, ...,\n",
       "         -1.49845739e-03, -1.22938072e-03,  1.05067354e-03]],\n",
       "\n",
       "       [[ 5.62852038e-05, -5.74638725e-05, -5.54467151e-05, ...,\n",
       "         -1.04926170e-04, -1.92459606e-06,  6.67352433e-05],\n",
       "        [-1.17833573e-04,  1.56431837e-04, -3.43148888e-04, ...,\n",
       "         -1.78693430e-04, -8.15506974e-06,  2.32813501e-04],\n",
       "        [-3.10414965e-04,  2.34317034e-04, -4.03908605e-04, ...,\n",
       "         -2.97813647e-04, -1.86450881e-04,  4.14301845e-04],\n",
       "        ...,\n",
       "        [ 1.36243831e-03,  1.98965846e-03,  4.62942495e-04, ...,\n",
       "         -1.57719955e-03, -2.03601597e-03,  2.47244630e-03],\n",
       "        [ 1.67350215e-03,  2.09536916e-03,  2.80935696e-04, ...,\n",
       "         -1.49162812e-03, -2.18102266e-03,  2.63106893e-03],\n",
       "        [ 1.96309318e-03,  2.17863079e-03,  1.01586906e-04, ...,\n",
       "         -1.37800490e-03, -2.28417688e-03,  2.78641749e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for enc_val, dec_val in dataset.take(1): break\n",
    "\n",
    "model(enc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f11ca27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eb9f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "609/609 [==============================] - 103s 166ms/step - loss: 3.4134\n",
      "Epoch 2/10\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.9694\n",
      "Epoch 3/10\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 2.8065\n",
      "Epoch 4/10\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.6798\n",
      "Epoch 5/10\n",
      "609/609 [==============================] - 99s 162ms/step - loss: 2.5703\n",
      "Epoch 6/10\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.4726\n",
      "Epoch 7/10\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.3821\n",
      "Epoch 8/10\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.2962\n",
      "Epoch 9/10\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.2155\n",
      "Epoch 10/10\n",
      "609/609 [==============================] - 99s 163ms/step - loss: 2.1396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9e404675e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam() \n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4709e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20): #시작 문자열을 init_sentence 로 받으며 디폴트값은 <start> 를 받는다\n",
    "    # init_sentence를 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence]) #텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True: #루프를 돌면서 init_sentence에 단어를 하나씩 생성\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4 \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a41b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyricist = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37c67f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m a liability <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3195664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#회고\n",
    "#- 이번 프로젝트에서 어려웠던 점\n",
    "#: 토크나이즈에서 결과값이 크게 좌우되었다. 되게 쉽게 보아 간과하고 있었다.\n",
    "#- 프로젝트를 진행하면서 알아낸 점 혹은 아직 모호한 점\n",
    "#: preprocessed_sentence에서 덜어낸 것과 sentence에서 덜어낸 것에 자수가 15자를 넘어가는 것에 특수문자에 포함된 것이냐가 크게 좌우했다.\n",
    "#이에 따라 학습하는 데이터의 질이 좋지 못하게 되어 결과값도 풍부하지 않게 나왔다.\n",
    "#- 루브릭 평가 지표를 맞추기 위해 시도한 것들:\n",
    "#: lf len(preprocessed_sentence.split(' '))<=15:\n",
    "#        corpus.append(preprocessed_sentence)\n",
    "#  if len(sentence.split(''))>15:\n",
    "#        continue\n",
    "#  두 방안을 사용하여 토큰화를 시도했다.\n",
    "#- 만약에 루브릭 평가 관련 지표를 달성 하지 못했을 때, 이유에 관한 추정:\n",
    "#: 달성했다.\n",
    "#- 자기 다짐:\n",
    "#: 하면 된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
